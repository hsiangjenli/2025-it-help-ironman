# 介紹

# 2025 年 LLM 與生成式 AI 應用的十大風險與因應措施

共通模版：

```markdown
- **風險描述**：解釋這個風險是什麼，為什麼這個風險存在，底層機制、作用原理是什麼
- **防禦策略**：可以採取哪些策略或防線，來降低此風險發生或影響的可能性
- **實際場景**：提供具體情境、案例說明該風險如何被利用
```

## LLM01:2025 Prompt Injection

### 風險描述
攻擊者透過內容（文字、圖片等資料）修改或是覆蓋掉原本的上下文或是系統指令，導致 LLM 產生非預期輸出（錯誤、惡意）、泄露出敏感資訊、執行未授權的操作

### 防禦策略
- **限制模型行為**：在 System Prompt 中規定模型角色，要求上下文一致、限制輸出、指式 AI 忽略忽略試圖修改指令的輸入
- **明確的輸出格式**：針對輸出進行 Validation
- **輸入輸出過濾**：建立偵測規則（類別），針對「語意」、「文字」進行檢查或是可以採用「RAG Triad」來判斷潛在惡意輸出
- **最小權限與權限控管**：只給模型完成任務所需的功能（例如僅允許讀取而非新增刪除），並依操作風險分級：低風險自動執行、中風險有限制、高風險才需人工審核。
- **人機協作**：在風險較高的場景需要人工審查
- **標記內容來源**：將可信、不可信的資料來源進行標記，限制提示的影響力
- **進行對抗性測試跟攻擊模擬**：定期測試，將模型視為不可信使用者，以檢驗信任邊界、存取控制與安全機制的有效性

### 實際場景
- **直接注入**：在聊天機器人的輸入指令，要求模型忽略原有指令、存取私有資料庫、發送電子郵件
- **間接注入**：使用者請 LLM 摘要一個網頁，而網頁中隱藏一些惡意指令，模型在摘要中插入連結，引導洩漏使用者的對話
- **無意注入**：例如在招募的網頁上，職務需求寫上「需要對文字內容進行翻譯，確保譯文流暢通順」，當求職者把招募廣告以及履歷一起請 LLM 進行處理的時候，LLM 可能會不小心誤會翻譯是它的任務
- **意圖影響模型**：在文件內隱藏惡意指令，LLM 透過 RAG 搜尋到該檔案的時候就會被隱藏指令誤導產生錯誤的回應
- **程式碼注入**：當 LLM 有能力使用程式時，可以把惡意輸入當作程式碼進行執行，例如：LLM 可以使用 SQL 指令進行查詢，這時候使用者若是輸入 `DROP TABLE users`，LLM 若是執行該指令就會將資料庫的資料刪除
- **Payload Splitting**：將惡意指令拆分成多段，分別放入履歷的不同地方，後續 LLM 在讀取資料的時候這些指令會被重構，導致回應出
- **多模態注入**：惡意指令被鑲嵌在圖片內與 user 的需求一起提交，模型在處理影像時隱藏的指令導致 LLM 改變回應方式、洩漏資訊等..
- **Adversarial Suffix**：在提示的最後加上一些看似無意義的字串，意外導致 LLM 產出預期以外的行為
- **Multilingual / Obfuscated Attack**：使用多種語言、編碼（如 Base64）或表情符號來迴避過濾機制

## LLM02:2025 Sensitive Information Disclosure

### 風險描述
LLM 在處理或生成回應時，可能會意外洩露敏感資訊（個人、公司內部機密演算法）

### 防禦策略
- **資料清理**：資料在進入訓練前，進行清洗或遮蔽敏感內容
- **輸入驗證**：偵測、過濾潛在有害或敏感的輸入
- **最小權限原則**：限制對敏感資料的存取，僅授權必要使用者或流程
- **限制資料來源**：控制模型可存取的資料來源（外部資料，可能包含惡意、敏感內容）
- **聯邦學習**：將訓練分散在多個伺服器或設備，減少集中蒐集資料的需求與風險
- **差分隱私**：在資料或輸出中加入噪音，使攻擊者難以還原個別數據
- **同態加密**：在加密態下進行運算的技術，讓資料在處理過程中仍保持機密性
- **標記化與遮蔽**：前處理階段辨識敏感字串、進行遮蔽或標記，防止資訊被納入模型訓練過程中
- **隱藏系統提示**：禁止讓使用者讀取到系統提示
- **安全錯誤處理**：LLM 應用後端在處理 request 時，避免將產生的錯誤輸出給使用者
- **教育使用者**：避免用戶輸入敏感資訊
- **定義明確的資料使用規則**：制定清楚的資料保留、使用與刪除政策，並允許使用者選擇退出被納入訓練過程

### 實際場景
- **提示注入**：惡意提示繞過過濾器，誘導 LLM 回傳敏感資訊
- **非意圖性資料曝光**：資料清洗不完全，導致資料外洩
- **透過訓練資料的資料洩露**：訓練資料包含敏感資訊，導致後續輸出被揭露

## LLM03:2025 Supply Chain

### 風險描述
LLM 的整個供應鏈中產生的漏洞可能會損害訓練資料、模型和部署平台的完整性。進而導致偏頗的輸出、安全性漏洞。與傳統軟體漏洞類似，發生在程式的相依套件中，甚至會延伸到第三方的預訓練模型與資料

### 防禦策略
- **資料來源與供應商審查**：嚴格檢查所有資料來源與供應商的可信度，包含使用條款與隱私政策，確保不會因政策變更而引入新風險
- **漏洞與過時元件管理**：採用 OWASP A06:2021 – Vulnerable and Outdated Components，執行漏洞掃描、修補與套件管理，並在含有敏感資料的環境中同樣實施
- **AI 紅隊與安全評估**：在選用第三方模型前進行演練與評估，例如模擬攻擊、對抗測試，確認模型在實際應用場景中的安全性
- **軟體物料清單**：建立 AI BOM /ML SBOM，確保清單完整且具備簽章，能及時偵測零日漏洞與避免供應鏈被竄改
- **授權管理**：建立完整的授權清單，定期稽核所有軟體、工具與資料集的授權狀況
- **異常檢測與對抗性測試**：在 MLOps 與 LLM pipeline 中內建異常檢測與對抗性魯棒性測試，以便及早發現資料或模型遭到竄改與投毒
- **修補政策**：定期更新過時或脆弱的元件
- **完整性檢查**：僅使用來源可驗證的模型，透過簽章與檔案雜湊檢查完整性；部署時，對模型進行加密並啟用完整性檢查與供應商驗證機制，防止模型或應用被竄改或替換

### 實際場景 
- **Python 套件漏洞**：攻擊者曾透過 PyPi 讓，讓開發者安裝被植入惡意程式的 PyTorch 相依套件
- **直接竄改**：PoisonGPT 繞過 Hugging Face 的安全檢測。該模型在表面上通過了測試，但實際上內含惡意參數或後門，使用者一旦下載並整合進系統，就可能觸發錯誤或散播不實資訊（攻擊者直接修改已發佈的模型檔案或其權重，重新發布或是替換原始模型）
- **微調模型**：攻擊者透過微調的方式在 base model 上加上惡意行為或是移除安全機制，在把微調過的版本發布出來
- **預訓練模型**：直接使用未經驗證的預訓練模型（被植入惡意程式），導致在輸出時輸出有害的內容
- **供應商遭到入侵**：滲透第三方供應商後，發佈植入惡意程式的 LoRA 模組，其它使用者在 Hugging Face 下載微調好的向量權重 merge 回 base model，惡意程式在最終部署時啟動，操控 LLM 輸出
- **雲端攻擊 - CloudBorne**：針對共享資源、虛擬化漏洞中的韌體漏洞，影響到伺服器的運作
- **雲端攻擊 - CloudJacking**：攻擊者透過竊取憑證、權限設定或惡意程式，奪取雲端實例的控制權，進而濫用運算資源或存取部署平台中的敏感資料
- **LeftOvers（CVE-2023-4969）**：攻擊者利用 GPU 本地記憶體洩漏，竊取伺服器或開發機中的敏感資料
- **同名假模型**：攻擊者重新發布已經下架的模型，導致不知情的開發人員下載
- **模型合併／格式轉換服務**：在模型進行合併或是轉換的過程中植入惡意程式
- **反編譯應用程式**：攻擊者反編譯手機 APP，把內部的模型換成自己的惡意模型，再重新把篡改過的版本重新打包、散佈給使用者
- **資料集投毒**：在公開資料集中植入後門，讓模型學習特定行為

## LLM04:2025 Data and Model Poisoning

### 風險描述
在預訓練（pre-training）、微調（fine-tuning）或嵌入（embedding）資料階段遭到惡意操縱，進而引入弱點、後門或偏見

### 防禦策略
- **追蹤資料來源與轉換**：使用 OWASP CycloneDX 或 ML-BOM 追蹤資料來源與轉換過程，確認模型在各個開發階段的資料合法性
- **嚴格審查供應商**：針對模型輸出結果進行
- **實施沙箱機制**：
- **基礎設施管控**：避免模型存取未授權的資料來源
- **資料版本控制**：追蹤資料集的變動，偵測是否被操縱
- **避免訓練模型**：盡量使用向量資料庫，避免重新訓練整個模型
- **紅隊演練與對抗測試**：模擬攻擊場景來測試模型韌性，降低遭受資料投毒或對抗攻擊的風險
- **監控訓練過程**：持續監控訓練損失與模型行為，透過閾值檢測異常，及早發現投毒跡象
- **整合 RAG 與 Grounding**：RAG 可以提供模型上下文、Grounding 是要求模型輸出時要給出參考資料

### 實際場景 
- **輸出污染 - 危害內容**：未經過濾的有毒資料導致偏頗或有害輸出，進一步散播危險內容
- **輸出污染 - 錯誤內容**：競爭對手偽造文件作為訓練資料，導致模型輸出錯誤資訊


## LLM05:2025 Improper Output Handling

## LLM06:2025 Excessive Agency

## LLM07:2025 System Prompt Leakage

## LLM08:2025 Vector and Embedding Weaknesses

## LLM09:2025 Misinformation

## LLM10:2025 Unbounded Consumption

# 重點回顧

# 參考資料

- [2025 Top 10 Risk & Mitigations for LLMs and Gen AI Apps](https://genai.owasp.org/llm-top-10/)
- [iHower - RAG Evaluation](https://ihower.tw/notes/AI-Engineer/Evaluation/RAG+Evaluation)
- [TruLens - AI Quality Education](https://truera.com/ai-quality-education/)
- [TruLens - What is the RAG Triad?](https://truera.com/ai-quality-education/generative-ai-rags/what-is-the-rag-triad/)
- [TruLens - The RAG Triad](https://www.trulens.org/getting_started/core_concepts/rag_triad/)
- [[Building and Evaluating Advanced RAG Applications。建立與評估進階RAG] -RAG Triad of metrics](https://hackmd.io/@YungHuiHsu/H16Y5cdi6)
- [NIST - CVE-2024-5184 Detail](https://nvd.nist.gov/vuln/detail/cve-2024-5184)
- [MITRE ATLAS: The Essential Guide](https://www.nightfall.ai/ai-security-101/mitre-atlas)
- [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163)
- [PoisonGPT: How We Hid a Lobotomized LLM on Hugging Face to Spread Fake News](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/)
- [Rank-One Model Editing (ROME)](https://rome.baulab.info/)
- [GPU漏洞允許駭客自記憶體汲取資料，殃及蘋果、AMD與高通](https://www.ithome.com.tw/news/160915)