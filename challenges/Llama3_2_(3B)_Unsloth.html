
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Unsloth + Llama3.2:3b 微調測試 &#8212; 2025 iThome 鐵人賽</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=d35f5538" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
    <script src="../_static/katex_autorenderer_pseudocode_algorithm.js?v=28afe2fa"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "displayMath": [["$$", "$$"], ["\\[", "\\]"]]}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'challenges/Llama3_2_(3B)_Unsloth';</script>
    <link rel="icon" href="https://hsiangjenli.github.io/static/image/ico.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="【Day14】使用 fastmcp 進行 MCP 開發" href="day-14.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">2025 iThome 鐵人賽</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">目錄:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="day-01.html">【Day01】爲自己定一個目標吧！</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-02.html">【Day02】LLM 專有名詞整理</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-03.html">【Day03】情境工程（Context Engineering）</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-04.html">【Day04】情境工程工具（10xrules）</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-05.html">【Day05】情境工程工具（coleam00/context-engineering-intro）</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-06.html">【Day06】情境工程工具（bmad-code-org/BMAD-METHOD）</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-07.html">【Day07】Ollama 介紹</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-08.html">【Day08】客製化 Ollama 模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-09.html">【Day09】Unsloth 模型微調方法</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-10.html">【Day10】實際使用 Unsloth 來對模型微調</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-11.html">【Day11】介紹 llms.txt</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-12.html">【Day12】介紹 MCP</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-13.html">【Day13】實際操作 MCP Server 並透過 GitHub Copilot 繪製架構圖</a></li>
<li class="toctree-l1"><a class="reference internal" href="day-14.html">【Day14】使用 fastmcp 進行 MCP 開發</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Jupyter Notebook</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Unsloth + Llama3.2:3b 微調測試</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/challenges/Llama3_2_(3B)_Unsloth.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Unsloth + Llama3.2:3b 微調測試</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#基本設定">基本設定</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#連接-Google-Drive">連接 Google Drive</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#安裝套件">安裝套件</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#使用-Unsloth-載入並初始化-4-bit-模型">使用 Unsloth 載入並初始化 4-bit 模型</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#載入-PEFT">載入 PEFT</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#資料集準備">資料集準備</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#訓練模型">訓練模型</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#訓練參數設定">訓練參數設定</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#資源查看">資源查看</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#開始微調">開始微調</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#查看花費時間">查看花費時間</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#運行微調後的模型結果">運行微調後的模型結果</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#儲存微調後的模型">儲存微調後的模型</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#轉換成-GGUF">轉換成 GGUF</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="Unsloth-+-Llama3.2:3b-微調測試">
<h1>Unsloth + Llama3.2:3b 微調測試<a class="headerlink" href="#Unsloth-+-Llama3.2:3b-微調測試" title="Link to this heading">#</a></h1>
<section id="基本設定">
<h2>基本設定<a class="headerlink" href="#基本設定" title="Link to this heading">#</a></h2>
<section id="連接-Google-Drive">
<h3>連接 Google Drive<a class="headerlink" href="#連接-Google-Drive" title="Link to this heading">#</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">google.colab</span><span class="w"> </span><span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Mounted at /content/drive
</pre></div></div>
</div>
</section>
<section id="安裝套件">
<h3>安裝套件<a class="headerlink" href="#安裝套件" title="Link to this heading">#</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">capture</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span><span class="o">,</span><span class="w"> </span><span class="nn">re</span>
<span class="k">if</span> <span class="s2">&quot;COLAB_&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">unsloth</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Do this only in Colab notebooks! Otherwise use pip install unsloth</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch</span><span class="p">;</span> <span class="n">v</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[0-9\.]{3,}&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">xformers</span> <span class="o">=</span> <span class="s2">&quot;xformers==&quot;</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;0.0.32.post2&quot;</span> <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="s2">&quot;2.8.0&quot;</span> <span class="k">else</span> <span class="s2">&quot;0.0.29.post3&quot;</span><span class="p">)</span>
    <span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">deps</span> <span class="n">bitsandbytes</span> <span class="n">accelerate</span> <span class="p">{</span><span class="n">xformers</span><span class="p">}</span> <span class="n">peft</span> <span class="n">trl</span> <span class="n">triton</span> <span class="n">cut_cross_entropy</span> <span class="n">unsloth_zoo</span>
    <span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">sentencepiece</span> <span class="n">protobuf</span> <span class="s2">&quot;datasets&gt;=3.4.1,&lt;4.0.0&quot;</span> <span class="s2">&quot;huggingface_hub&gt;=0.34.0&quot;</span> <span class="n">hf_transfer</span>
    <span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">deps</span> <span class="n">unsloth</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span><span class="o">==</span><span class="mf">4.55.4</span>
</pre></div>
</div>
</div>
</section>
<section id="使用-Unsloth-載入並初始化-4-bit-模型">
<h3>使用 Unsloth 載入並初始化 4-bit 模型<a class="headerlink" href="#使用-Unsloth-載入並初始化-4-bit-模型" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>這段程式碼使用 Unsloth 框架載入 LLM</p></li>
<li><p>透過 FastLanguageModel.from_pretrained() 下載並初始化模型與 tokenizer</p></li>
<li><p>dtype=None 會自動依 GPU 選擇最佳精度 (FP16 / BF16)</p></li>
<li><p>load_in_4bit=True 表示使用 4-bit 量化，降低 VRAM 占用，加快載入與推論速度</p></li>
<li><p>fourbit_models 列出了官方預先量化的模型，下載更快、更省資源</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">unsloth</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastLanguageModel</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">2048</span> <span class="c1"># Choose any! We auto support RoPE Scaling internally!</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span>
<span class="n">load_in_4bit</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># Use 4bit quantization to reduce memory usage. Can be False.</span>

<span class="c1"># 4bit pre quantized models we support for 4x faster downloading + no OOMs.</span>
<span class="n">fourbit_models</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;unsloth/Meta-Llama-3.1-8B-bnb-4bit&quot;</span><span class="p">,</span>      <span class="c1"># Llama-3.1 2x faster</span>
    <span class="s2">&quot;unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsloth/Meta-Llama-3.1-70B-bnb-4bit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsloth/Meta-Llama-3.1-405B-bnb-4bit&quot;</span><span class="p">,</span>    <span class="c1"># 4bit for 405b!</span>
    <span class="s2">&quot;unsloth/Mistral-Small-Instruct-2409&quot;</span><span class="p">,</span>     <span class="c1"># Mistral 22b 2x faster!</span>
    <span class="s2">&quot;unsloth/mistral-7b-instruct-v0.3-bnb-4bit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsloth/Phi-3.5-mini-instruct&quot;</span><span class="p">,</span>           <span class="c1"># Phi-3.5 2x faster!</span>
    <span class="s2">&quot;unsloth/Phi-3-medium-4k-instruct&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsloth/gemma-2-9b-bnb-4bit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsloth/gemma-2-27b-bnb-4bit&quot;</span><span class="p">,</span>            <span class="c1"># Gemma 2x faster!</span>

    <span class="s2">&quot;unsloth/Llama-3.2-1B-bnb-4bit&quot;</span><span class="p">,</span>           <span class="c1"># NEW! Llama 3.2 models</span>
    <span class="s2">&quot;unsloth/Llama-3.2-1B-Instruct-bnb-4bit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsloth/Llama-3.2-3B-bnb-4bit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsloth/Llama-3.2-3B-Instruct-bnb-4bit&quot;</span><span class="p">,</span>

    <span class="s2">&quot;unsloth/Llama-3.3-70B-Instruct-bnb-4bit&quot;</span> <span class="c1"># NEW! Llama 3.3 70B!</span>
<span class="p">]</span> <span class="c1"># More models at https://huggingface.co/unsloth</span>

<span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;unsloth/Llama-3.2-3B-Instruct&quot;</span><span class="p">,</span> <span class="c1"># or choose &quot;unsloth/Llama-3.2-1B-Instruct&quot;</span>
    <span class="n">max_seq_length</span> <span class="o">=</span> <span class="n">max_seq_length</span><span class="p">,</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">,</span>
    <span class="n">load_in_4bit</span> <span class="o">=</span> <span class="n">load_in_4bit</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.55.4.
   \\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]
 &#34;-____-&#34;     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "04003b7454e749a58f6bbae303531d01"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "7a1311df3aab46efb46bc8b82426b410"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "8f09b80b9ca940ee847c154fb250794d"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "882feda1f44647b3be7ca704a87245ae"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "7eb1b56493eb46d88f8d8c239ba41a05"}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "ec420f89562149f5be73ac515ec2a0d6"}</script></div>
</div>
</section>
<section id="載入-PEFT">
<h3>載入 PEFT<a class="headerlink" href="#載入-PEFT" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>FastLanguageModel.get_peft_model：這裡把 base model 套上 LoRA adapter</p></li>
<li><p>r: rank（越高越準、越耗記憶體）；lora_alpha: scaling；lora_dropout: 避免過擬合；</p></li>
<li><p>target_modules: 針對 Attention/MLP 的哪些 Linear 層套 LoRA（依模型結構而定）</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">get_peft_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">r</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="c1"># Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128</span>
    <span class="n">target_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;o_proj&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;gate_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;up_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;down_proj&quot;</span><span class="p">,],</span>
    <span class="n">lora_alpha</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_dropout</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="c1"># Supports any, but = 0 is optimized</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>    <span class="c1"># Supports any, but = &quot;none&quot; is optimized</span>
    <span class="n">use_gradient_checkpointing</span> <span class="o">=</span> <span class="s2">&quot;unsloth&quot;</span><span class="p">,</span> <span class="c1"># True or &quot;unsloth&quot; for very long context</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="mi">3407</span><span class="p">,</span>
    <span class="n">use_rslora</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># We support rank stabilized LoRA</span>
    <span class="n">loftq_config</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># And LoftQ</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Unsloth 2025.9.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.
</pre></div></div>
</div>
</section>
</section>
<section id="資料集準備">
<h2>資料集準備<a class="headerlink" href="#資料集準備" title="Link to this heading">#</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">hsiangjenli</span><span class="o">.</span><span class="n">github</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="mi">2025</span><span class="o">-</span><span class="n">it</span><span class="o">-</span><span class="n">help</span><span class="o">-</span><span class="n">ironman</span><span class="o">/</span><span class="n">_static</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">data</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">O</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span><span class="n">data</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">unsloth.chat_templates</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_chat_template</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_chat_template</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">chat_template</span> <span class="o">=</span> <span class="s2">&quot;llama-3.1&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">formatting_prompts_func</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">convos</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;conversations&quot;</span><span class="p">]</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">convo</span><span class="p">,</span> <span class="n">tokenize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">convo</span> <span class="ow">in</span> <span class="n">convos</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">{</span> <span class="s2">&quot;text&quot;</span> <span class="p">:</span> <span class="n">texts</span><span class="p">,</span> <span class="p">}</span>
<span class="k">pass</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">unsloth.chat_templates</span><span class="w"> </span><span class="kn">import</span> <span class="n">standardize_sharegpt</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span>
    <span class="s2">&quot;json&quot;</span><span class="p">,</span>
    <span class="n">data_files</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="s2">&quot;/content/data.json&quot;</span><span class="p">},</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span>
<span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">to_conversations</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;conversations&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;from&quot;</span><span class="p">:</span> <span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;eng&quot;</span><span class="p">]},</span>
            <span class="p">{</span><span class="s2">&quot;from&quot;</span><span class="p">:</span> <span class="s2">&quot;gpt&quot;</span><span class="p">,</span>   <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;zh-tw&quot;</span><span class="p">]},</span>
        <span class="p">]</span>
    <span class="p">}</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">to_conversations</span><span class="p">)</span>

<span class="c1"># 轉換成 standardize_sharegpt 格式，格式：(&quot;role&quot;,&quot;content&quot;)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">standardize_sharegpt</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># 建立 chat template，與官方教學一致用 conversations 來產出 text 欄位</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;unsloth/llama-3.2-3b-bnb-4bit&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_chat_template</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">chat_template</span><span class="o">=</span><span class="s2">&quot;llama-3.1&quot;</span><span class="p">)</span>  <span class="c1"># 依你的模型調整</span>

<span class="k">def</span><span class="w"> </span><span class="nf">formatting_prompts_func</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">convos</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;conversations&quot;</span><span class="p">]</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
            <span class="n">convo</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">convo</span> <span class="ow">in</span> <span class="n">convos</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">texts</span><span class="p">}</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">formatting_prompts_func</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "69dad75c802f4bb990b2774a15ce8f96"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "db732be82b784dca8630f5885cf3bcd5"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "c3b32ab5a48c4b3cbe595d8b4054d2da"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "8359d948670c449ea331465c5f2c5d0a"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "507e6f84ff9c48b8af8d9562a467ecb6"}</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "22af7643dd6a492f843a359195d4d0c8"}</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "4ed46481cb0249f49545135d1549d906"}</script></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nCutting Knowledge Date: December 2023\nToday Date: 26 July 2024\n\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n**Iterators terminating on the shortest input sequence:**&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n**在最短輸入序列 (shortest input sequence) 處終止的疊代器：**&lt;|eot_id|&gt;&#39;
</pre></div></div>
</div>
</section>
<section id="訓練模型">
<h2>訓練模型<a class="headerlink" href="#訓練模型" title="Link to this heading">#</a></h2>
<section id="訓練參數設定">
<h3>訓練參數設定<a class="headerlink" href="#訓練參數設定" title="Link to this heading">#</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">SFTConfig</span><span class="p">,</span> <span class="n">SFTTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataCollatorForSeq2Seq</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">,</span>
    <span class="n">dataset_text_field</span> <span class="o">=</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span>
    <span class="n">max_seq_length</span> <span class="o">=</span> <span class="n">max_seq_length</span><span class="p">,</span>
    <span class="n">data_collator</span> <span class="o">=</span> <span class="n">DataCollatorForSeq2Seq</span><span class="p">(</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">),</span>
    <span class="n">packing</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># Can make training 5x faster for short sequences.</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">SFTConfig</span><span class="p">(</span>
        <span class="n">per_device_train_batch_size</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">warmup_steps</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="c1"># num_train_epochs = 1, # Set this for 1 full training run.</span>
        <span class="n">max_steps</span> <span class="o">=</span> <span class="mi">60</span><span class="p">,</span>
        <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">2e-4</span><span class="p">,</span>
        <span class="n">logging_steps</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">optim</span> <span class="o">=</span> <span class="s2">&quot;adamw_8bit&quot;</span><span class="p">,</span>
        <span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">lr_scheduler_type</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="mi">3407</span><span class="p">,</span>
        <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;outputs&quot;</span><span class="p">,</span>
        <span class="n">report_to</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="c1"># Use this for WandB etc</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "8b812f0405c24060b4d3ae256ad0db78"}</script></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">unsloth.chat_templates</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_on_responses_only</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">train_on_responses_only</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">,</span>
    <span class="n">instruction_part</span> <span class="o">=</span> <span class="s2">&quot;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">response_part</span> <span class="o">=</span> <span class="s2">&quot;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "cebf494ade2e4b47b3a076731c3e3ce7"}</script></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 查看轉換過後的結果</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">][</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;&lt;|begin_of_text|&gt;&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nCutting Knowledge Date: December 2023\nToday Date: 26 July 2024\n\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n**Iterators terminating on the shortest input sequence:**&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n**在最短輸入序列 (shortest input sequence) 處終止的疊代器：**&lt;|eot_id|&gt;&#39;
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">space</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">space</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="o">-</span><span class="mi">100</span> <span class="k">else</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">][</span><span class="s2">&quot;labels&quot;</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;                                              **在最短輸入序列 (shortest input sequence) 處終止的疊代器：**&lt;|eot_id|&gt;&#39;
</pre></div></div>
</div>
</section>
<section id="資源查看">
<h3>資源查看<a class="headerlink" href="#資源查看" title="Link to this heading">#</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gpu_stats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">start_gpu_memory</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">max_memory</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">gpu_stats</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU = </span><span class="si">{</span><span class="n">gpu_stats</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">. Max memory = </span><span class="si">{</span><span class="n">max_memory</span><span class="si">}</span><span class="s2"> GB.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">start_gpu_memory</span><span class="si">}</span><span class="s2"> GB of memory reserved.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
GPU = Tesla T4. Max memory = 14.741 GB.
3.07 GB of memory reserved.
</pre></div></div>
</div>
</section>
<section id="開始微調">
<h3>開始微調<a class="headerlink" href="#開始微調" title="Link to this heading">#</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer_stats</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 74 | Num Epochs = 6 | Total steps = 60
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 &#34;-____-&#34;     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Unsloth: Will smartly offload gradients to save VRAM!
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
    <div>

      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [60/60 02:35, Epoch 6/6]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>2.099300</td>
    </tr>
    <tr>
      <td>2</td>
      <td>2.319000</td>
    </tr>
    <tr>
      <td>3</td>
      <td>2.232800</td>
    </tr>
    <tr>
      <td>4</td>
      <td>2.140000</td>
    </tr>
    <tr>
      <td>5</td>
      <td>2.085500</td>
    </tr>
    <tr>
      <td>6</td>
      <td>2.246200</td>
    </tr>
    <tr>
      <td>7</td>
      <td>1.538600</td>
    </tr>
    <tr>
      <td>8</td>
      <td>1.617900</td>
    </tr>
    <tr>
      <td>9</td>
      <td>2.000900</td>
    </tr>
    <tr>
      <td>10</td>
      <td>2.164600</td>
    </tr>
    <tr>
      <td>11</td>
      <td>1.288900</td>
    </tr>
    <tr>
      <td>12</td>
      <td>1.229000</td>
    </tr>
    <tr>
      <td>13</td>
      <td>1.114200</td>
    </tr>
    <tr>
      <td>14</td>
      <td>1.087900</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.970900</td>
    </tr>
    <tr>
      <td>16</td>
      <td>1.049800</td>
    </tr>
    <tr>
      <td>17</td>
      <td>1.039700</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.754300</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.766500</td>
    </tr>
    <tr>
      <td>20</td>
      <td>1.161000</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.624900</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.547300</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.836900</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.599400</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.602800</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.555900</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.619000</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.496500</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.567800</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.258900</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.287000</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.321900</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.481500</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.334100</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.389700</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.337600</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.357400</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.308100</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.365500</td>
    </tr>
    <tr>
      <td>40</td>
      <td>0.395700</td>
    </tr>
    <tr>
      <td>41</td>
      <td>0.161800</td>
    </tr>
    <tr>
      <td>42</td>
      <td>0.222900</td>
    </tr>
    <tr>
      <td>43</td>
      <td>0.202700</td>
    </tr>
    <tr>
      <td>44</td>
      <td>0.240600</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.274600</td>
    </tr>
    <tr>
      <td>46</td>
      <td>0.201500</td>
    </tr>
    <tr>
      <td>47</td>
      <td>0.323600</td>
    </tr>
    <tr>
      <td>48</td>
      <td>0.203300</td>
    </tr>
    <tr>
      <td>49</td>
      <td>0.204800</td>
    </tr>
    <tr>
      <td>50</td>
      <td>0.101700</td>
    </tr>
    <tr>
      <td>51</td>
      <td>0.157600</td>
    </tr>
    <tr>
      <td>52</td>
      <td>0.115400</td>
    </tr>
    <tr>
      <td>53</td>
      <td>0.143100</td>
    </tr>
    <tr>
      <td>54</td>
      <td>0.217000</td>
    </tr>
    <tr>
      <td>55</td>
      <td>0.141400</td>
    </tr>
    <tr>
      <td>56</td>
      <td>0.202100</td>
    </tr>
    <tr>
      <td>57</td>
      <td>0.078900</td>
    </tr>
    <tr>
      <td>58</td>
      <td>0.156400</td>
    </tr>
    <tr>
      <td>59</td>
      <td>0.126000</td>
    </tr>
    <tr>
      <td>60</td>
      <td>0.220900</td>
    </tr>
  </tbody>
</table><p></div>
</div>
</section>
<section id="查看花費時間">
<h3>查看花費時間<a class="headerlink" href="#查看花費時間" title="Link to this heading">#</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">used_memory</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">used_memory_for_lora</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">used_memory</span> <span class="o">-</span> <span class="n">start_gpu_memory</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">used_percentage</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">used_memory</span> <span class="o">/</span> <span class="n">max_memory</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">lora_percentage</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">used_memory_for_lora</span> <span class="o">/</span> <span class="n">max_memory</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trainer_stats</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;train_runtime&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> seconds used for training.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">trainer_stats</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;train_runtime&#39;</span><span class="p">]</span><span class="o">/</span><span class="mi">60</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> minutes used for training.&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak reserved memory = </span><span class="si">{</span><span class="n">used_memory</span><span class="si">}</span><span class="s2"> GB.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak reserved memory for training = </span><span class="si">{</span><span class="n">used_memory_for_lora</span><span class="si">}</span><span class="s2"> GB.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak reserved memory % of max memory = </span><span class="si">{</span><span class="n">used_percentage</span><span class="si">}</span><span class="s2"> %.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak reserved memory for training % of max memory = </span><span class="si">{</span><span class="n">lora_percentage</span><span class="si">}</span><span class="s2"> %.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
175.4575 seconds used for training.
2.92 minutes used for training.
Peak reserved memory = 3.119 GB.
Peak reserved memory for training = 0.049 GB.
Peak reserved memory % of max memory = 21.159 %.
Peak reserved memory for training % of max memory = 0.332 %.
</pre></div></div>
</div>
</section>
<section id="運行微調後的模型結果">
<h3>運行微調後的模型結果<a class="headerlink" href="#運行微調後的模型結果" title="Link to this heading">#</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">for_inference</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># Enable native 2x faster inference</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;timer file descriptor HOWTO&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">tokenize</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">add_generation_prompt</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># Must add for generation</span>
    <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TextStreamer</span>
<span class="n">text_streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">skip_prompt</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">streamer</span> <span class="o">=</span> <span class="n">text_streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
                   <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">min_p</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input&#39;s `attention_mask` to obtain reliable results.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
定時器文件 descriptor教材&lt;|eot_id|&gt;
</pre></div></div>
</div>
</section>
</section>
<section id="儲存微調後的模型">
<h2>儲存微調後的模型<a class="headerlink" href="#儲存微調後的模型" title="Link to this heading">#</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;/content/drive/MyDrive/lora_model&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;/content/drive/MyDrive/lora_model&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(&#39;/content/drive/MyDrive/lora_model/tokenizer_config.json&#39;,
 &#39;/content/drive/MyDrive/lora_model/special_tokens_map.json&#39;,
 &#39;/content/drive/MyDrive/lora_model/chat_template.jinja&#39;,
 &#39;/content/drive/MyDrive/lora_model/tokenizer.json&#39;)
</pre></div></div>
</div>
</section>
<section id="轉換成-GGUF">
<h2>轉換成 GGUF<a class="headerlink" href="#轉換成-GGUF" title="Link to this heading">#</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">git</span> <span class="n">clone</span> <span class="o">--</span><span class="n">recursive</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">ggerganov</span><span class="o">/</span><span class="n">llama</span><span class="o">.</span><span class="n">cpp</span>
<span class="err">!</span><span class="p">(</span><span class="n">cd</span> <span class="n">llama</span><span class="o">.</span><span class="n">cpp</span><span class="p">;</span> <span class="n">cmake</span> <span class="o">-</span><span class="n">B</span> <span class="n">build</span><span class="p">;</span><span class="n">cmake</span> <span class="o">--</span><span class="n">build</span> <span class="n">build</span> <span class="o">--</span><span class="n">config</span> <span class="n">Release</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cloning into &#39;llama.cpp&#39;...
remote: Enumerating objects: 61208, done.
remote: Counting objects: 100% (153/153), done.
remote: Compressing objects: 100% (88/88), done.
remote: Total 61208 (delta 99), reused 72 (delta 65), pack-reused 61055 (from 3)
Receiving objects: 100% (61208/61208), 151.79 MiB | 17.08 MiB/s, done.
Resolving deltas: 100% (44461/44461), done.
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
CMAKE_BUILD_TYPE=Release
-- Found Git: /usr/bin/git (found version &#34;2.34.1&#34;)
-- The ASM compiler identification is GNU
-- Found assembler: /usr/bin/cc
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- GGML_SYSTEM_ARCH: x86
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version &#34;4.5&#34;)
-- Found OpenMP_CXX: -fopenmp (found version &#34;4.5&#34;)
-- Found OpenMP: TRUE (found version &#34;4.5&#34;)
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native
-- ggml version: 0.0.6403
-- ggml commit:  3b15924d
-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version &#34;7.81.0&#34;)
-- Configuring done (1.3s)
-- Generating done (0.2s)
-- Build files have been written to: /content/llama.cpp/build
[  1%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o</span>
[  1%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o</span>
[  2%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o</span>
[  2%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o</span>
[  2%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o</span>
[  3%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o</span>
[  3%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o</span>
[  4%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o</span>
[  4%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX shared library ../../bin/libggml-base.so</span>
[  4%] Built target ggml-base
[  4%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o</span>
[  5%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o</span>
[  5%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o</span>
[  6%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o</span>
[  6%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o</span>
[  6%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o</span>
[  7%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o</span>
[  7%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o</span>
[  8%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o</span>
[  8%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o</span>
[  8%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o</span>
[  9%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o</span>
[  9%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o</span>
[ 10%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o</span>
[ 10%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o</span>
[ 10%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX shared library ../../bin/libggml-cpu.so</span>
[ 10%] Built target ggml-cpu
[ 10%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o</span>
[ 10%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX shared library ../../bin/libggml.so</span>
[ 10%] Built target ggml
[ 11%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o</span>
[ 11%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o</span>
[ 12%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o</span>
[ 12%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o</span>
[ 12%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o</span>
[ 13%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o</span>
[ 13%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o</span>
[ 14%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o</span>
[ 14%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o</span>
[ 14%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o</span>
[ 15%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o</span>
[ 15%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o</span>
[ 16%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o</span>
[ 16%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o</span>
[ 16%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o</span>
[ 17%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o</span>
[ 17%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o</span>
[ 18%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o</span>
[ 18%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o</span>
[ 18%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o</span>
[ 19%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o</span>
[ 19%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o</span>
[ 20%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o</span>
[ 20%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o</span>
[ 20%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o</span>
[ 21%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o</span>
[ 21%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX shared library ../bin/libllama.so</span>
[ 21%] Built target llama
[ 21%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o</span>
[ 21%] Built target build_info
[ 21%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/arg.cpp.o</span>
[ 22%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o</span>
[ 22%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/chat.cpp.o</span>
[ 23%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/common.cpp.o</span>
[ 23%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/console.cpp.o</span>
[ 23%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/json-partial.cpp.o</span>
[ 24%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o</span>
[ 24%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o</span>
[ 25%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/log.cpp.o</span>
[ 25%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o</span>
[ 25%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o</span>
[ 26%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o</span>
[ 26%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o</span>
[ 27%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX static library libcommon.a</span>
[ 27%] Built target common
[ 27%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o</span>
[ 27%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-tokenizer-0</span>
[ 27%] Built target test-tokenizer-0
[ 28%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o</span>
[ 28%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o</span>
[ 28%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-sampling</span>
[ 28%] Built target test-sampling
[ 28%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o</span>
[ 29%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o</span>
[ 29%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-grammar-parser</span>
[ 29%] Built target test-grammar-parser
[ 29%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o</span>
[ 30%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o</span>
[ 30%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-grammar-integration</span>
[ 30%] Built target test-grammar-integration
[ 30%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o</span>
[ 30%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o</span>
[ 31%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-llama-grammar</span>
[ 31%] Built target test-llama-grammar
[ 32%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o</span>
[ 32%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o</span>
[ 32%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-chat</span>
[ 32%] Built target test-chat
[ 33%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o</span>
[ 33%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o</span>
[ 34%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-json-schema-to-grammar</span>
[ 34%] Built target test-json-schema-to-grammar
[ 34%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o</span>
[ 35%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-quantize-stats</span>
[ 35%] Built target test-quantize-stats
[ 35%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o</span>
[ 36%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-gbnf-validator</span>
[ 36%] Built target test-gbnf-validator
[ 37%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o</span>
[ 37%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-tokenizer-1-bpe</span>
[ 37%] Built target test-tokenizer-1-bpe
[ 38%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o</span>
[ 38%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-tokenizer-1-spm</span>
[ 38%] Built target test-tokenizer-1-spm
[ 39%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o</span>
[ 39%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o</span>
[ 40%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-chat-parser</span>
[ 40%] Built target test-chat-parser
[ 40%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o</span>
[ 40%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o</span>
[ 41%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-chat-template</span>
[ 41%] Built target test-chat-template
[ 42%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o</span>
[ 42%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o</span>
[ 42%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-json-partial</span>
[ 42%] Built target test-json-partial
[ 42%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o</span>
[ 43%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o</span>
[ 43%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-log</span>
[ 43%] Built target test-log
[ 43%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o</span>
[ 44%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o</span>
[ 44%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-regex-partial</span>
[ 44%] Built target test-regex-partial
[ 45%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o</span>
[ 45%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o</span>
[ 46%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-thread-safety</span>
[ 46%] Built target test-thread-safety
[ 46%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o</span>
[ 46%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o</span>
[ 47%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-arg-parser</span>
[ 47%] Built target test-arg-parser
[ 48%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o</span>
[ 48%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o</span>
[ 49%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-opt</span>
[ 49%] Built target test-opt
[ 49%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o</span>
[ 49%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o</span>
[ 50%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-gguf</span>
[ 50%] Built target test-gguf
[ 50%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o</span>
[ 51%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o</span>
[ 51%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-backend-ops</span>
[ 51%] Built target test-backend-ops
[ 51%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o</span>
[ 52%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o</span>
[ 52%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-model-load-cancel</span>
[ 52%] Built target test-model-load-cancel
[ 52%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o</span>
[ 53%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o</span>
[ 53%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-autorelease</span>
[ 53%] Built target test-autorelease
[ 54%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o</span>
[ 54%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o</span>
[ 54%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-barrier</span>
[ 54%] Built target test-barrier
[ 54%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o</span>
[ 54%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o</span>
[ 55%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-quantize-fns</span>
[ 55%] Built target test-quantize-fns
[ 55%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o</span>
[ 56%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o</span>
[ 56%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-quantize-perf</span>
[ 56%] Built target test-quantize-perf
[ 56%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o</span>
[ 57%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o</span>
[ 57%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-rope</span>
[ 57%] Built target test-rope
[ 57%] <span class="ansi-green-fg">Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o</span>
[ 58%] <span class="ansi-green-fg">Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o</span>
[ 58%] <span class="ansi-green-fg">Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o</span>
[ 58%] <span class="ansi-green-fg">Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o</span>
[ 59%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX shared library ../../bin/libmtmd.so</span>
[ 59%] Built target mtmd
[ 60%] <span class="ansi-green-fg">Building C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o</span>
[ 60%] <span class="ansi-green-fg">Building CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o</span>
[ 60%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../bin/test-mtmd-c-api</span>
[ 60%] Built target test-mtmd-c-api
[ 61%] <span class="ansi-green-fg">Building C object tests/CMakeFiles/test-c.dir/test-c.c.o</span>
[ 61%] <span class="ansi-green-intense-fg ansi-bold">Linking C executable ../bin/test-c</span>
[ 61%] Built target test-c
[ 62%] <span class="ansi-green-fg">Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o</span>
[ 62%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-batched</span>
[ 62%] Built target llama-batched
[ 62%] <span class="ansi-green-fg">Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o</span>
[ 63%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-embedding</span>
[ 63%] Built target llama-embedding
[ 63%] <span class="ansi-green-fg">Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o</span>
[ 63%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-eval-callback</span>
[ 63%] Built target llama-eval-callback
[ 64%] <span class="ansi-green-fg">Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o</span>
[ 64%] Built target sha256
[ 65%] <span class="ansi-green-fg">Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o</span>
[ 65%] Built target xxhash
[ 65%] <span class="ansi-green-fg">Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o</span>
[ 65%] Built target sha1
[ 66%] <span class="ansi-green-fg">Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o</span>
[ 66%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-gguf-hash</span>
[ 66%] Built target llama-gguf-hash
[ 66%] <span class="ansi-green-fg">Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o</span>
[ 66%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-gguf</span>
[ 66%] Built target llama-gguf
[ 66%] <span class="ansi-green-fg">Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o</span>
[ 67%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-gritlm</span>
[ 67%] Built target llama-gritlm
[ 68%] <span class="ansi-green-fg">Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o</span>
[ 68%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-lookahead</span>
[ 68%] Built target llama-lookahead
[ 68%] <span class="ansi-green-fg">Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o</span>
[ 69%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-lookup</span>
[ 69%] Built target llama-lookup
[ 69%] <span class="ansi-green-fg">Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o</span>
[ 70%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-lookup-create</span>
[ 70%] Built target llama-lookup-create
[ 70%] <span class="ansi-green-fg">Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o</span>
[ 70%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-lookup-merge</span>
[ 70%] Built target llama-lookup-merge
[ 71%] <span class="ansi-green-fg">Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o</span>
[ 71%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-lookup-stats</span>
[ 71%] Built target llama-lookup-stats
[ 71%] <span class="ansi-green-fg">Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o</span>
[ 72%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-parallel</span>
[ 72%] Built target llama-parallel
[ 72%] <span class="ansi-green-fg">Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o</span>
[ 73%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-passkey</span>
[ 73%] Built target llama-passkey
[ 73%] <span class="ansi-green-fg">Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o</span>
[ 74%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-retrieval</span>
[ 74%] Built target llama-retrieval
[ 74%] <span class="ansi-green-fg">Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o</span>
[ 75%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-save-load-state</span>
[ 75%] Built target llama-save-load-state
[ 76%] <span class="ansi-green-fg">Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o</span>
[ 76%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-simple</span>
[ 76%] Built target llama-simple
[ 76%] <span class="ansi-green-fg">Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o</span>
[ 77%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-simple-chat</span>
[ 77%] Built target llama-simple-chat
[ 77%] <span class="ansi-green-fg">Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o</span>
[ 78%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-speculative</span>
[ 78%] Built target llama-speculative
[ 78%] <span class="ansi-green-fg">Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o</span>
[ 78%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-speculative-simple</span>
[ 78%] Built target llama-speculative-simple
[ 78%] <span class="ansi-green-fg">Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o</span>
[ 79%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-gen-docs</span>
[ 79%] Built target llama-gen-docs
[ 80%] <span class="ansi-green-fg">Building CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o</span>
[ 80%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-finetune</span>
[ 80%] Built target llama-finetune
[ 80%] <span class="ansi-green-fg">Building CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o</span>
[ 81%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-diffusion-cli</span>
[ 81%] Built target llama-diffusion-cli
[ 82%] <span class="ansi-green-fg">Building CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o</span>
[ 82%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-logits</span>
[ 82%] Built target llama-logits
[ 83%] <span class="ansi-green-fg">Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o</span>
[ 83%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml</span>
[ 83%] Built target llama-convert-llama2c-to-ggml
[ 83%] <span class="ansi-green-fg">Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o</span>
[ 84%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-vdot</span>
[ 84%] Built target llama-vdot
[ 85%] <span class="ansi-green-fg">Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o</span>
[ 85%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-q8dot</span>
[ 85%] Built target llama-q8dot
[ 85%] <span class="ansi-green-fg">Building CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o</span>
[ 86%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-batched-bench</span>
[ 86%] Built target llama-batched-bench
[ 87%] <span class="ansi-green-fg">Building CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o</span>
[ 87%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-gguf-split</span>
[ 87%] Built target llama-gguf-split
[ 87%] <span class="ansi-green-fg">Building CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o</span>
[ 88%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-imatrix</span>
[ 88%] Built target llama-imatrix
[ 88%] <span class="ansi-green-fg">Building CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o</span>
[ 89%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-bench</span>
[ 89%] Built target llama-bench
[ 89%] <span class="ansi-green-fg">Building CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o</span>
[ 89%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-cli</span>
[ 89%] Built target llama-cli
[ 89%] <span class="ansi-green-fg">Building CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o</span>
[ 89%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-perplexity</span>
[ 89%] Built target llama-perplexity
[ 90%] <span class="ansi-green-fg">Building CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o</span>
[ 90%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-quantize</span>
[ 90%] Built target llama-quantize
[ 90%] <span class="ansi-blue-intense-fg ansi-bold">Generating loading.html.hpp</span>
[ 90%] <span class="ansi-blue-intense-fg ansi-bold">Generating index.html.gz.hpp</span>
[ 91%] <span class="ansi-green-fg">Building CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o</span>
[ 91%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-server</span>
[ 91%] Built target llama-server
[ 91%] <span class="ansi-green-fg">Building CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o</span>
[ 91%] <span class="ansi-green-fg">Building CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o</span>
[ 92%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-run</span>
[ 92%] Built target llama-run
[ 93%] <span class="ansi-green-fg">Building CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o</span>
[ 93%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-tokenize</span>
[ 93%] Built target llama-tokenize
[ 94%] <span class="ansi-green-fg">Building CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o</span>
[ 94%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-tts</span>
[ 94%] Built target llama-tts
[ 94%] <span class="ansi-green-fg">Building CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o</span>
[ 94%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-llava-cli</span>
[ 94%] Built target llama-llava-cli
[ 94%] <span class="ansi-green-fg">Building CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o</span>
[ 95%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-gemma3-cli</span>
[ 95%] Built target llama-gemma3-cli
[ 96%] <span class="ansi-green-fg">Building CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o</span>
[ 96%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-minicpmv-cli</span>
[ 96%] Built target llama-minicpmv-cli
[ 96%] <span class="ansi-green-fg">Building CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o</span>
[ 97%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-qwen2vl-cli</span>
[ 97%] Built target llama-qwen2vl-cli
[ 97%] <span class="ansi-green-fg">Building CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o</span>
[ 98%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-mtmd-cli</span>
[ 98%] Built target llama-mtmd-cli
[ 99%] <span class="ansi-green-fg">Building CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o</span>
[ 99%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-cvector-generator</span>
[ 99%] Built target llama-cvector-generator
[100%] <span class="ansi-green-fg">Building CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o</span>
[100%] <span class="ansi-green-intense-fg ansi-bold">Linking CXX executable ../../bin/llama-export-lora</span>
[100%] Built target llama-export-lora
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="o">.</span><span class="n">githubusercontent</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">ggml</span><span class="o">-</span><span class="n">org</span><span class="o">/</span><span class="n">llama</span><span class="o">.</span><span class="n">cpp</span><span class="o">/</span><span class="n">refs</span><span class="o">/</span><span class="n">heads</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">convert_hf_to_gguf</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">O</span> <span class="n">llama</span><span class="o">.</span><span class="n">cpp</span><span class="o">/</span><span class="n">convert_hf_to_gguf</span><span class="o">.</span><span class="n">py</span>
<span class="k">if</span> <span class="kc">True</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">save_pretrained_gguf</span><span class="p">(</span><span class="s2">&quot;/content/drive/MyDrive/lora_model/q4_k_m&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">quantization_method</span> <span class="o">=</span> <span class="s2">&quot;q4_k_m&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
--2025-09-07 16:06:57--  https://raw.githubusercontent.com/ggml-org/llama.cpp/refs/heads/master/convert_hf_to_gguf.py
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 419460 (410K) [text/plain]
Saving to: ‘llama.cpp/convert_hf_to_gguf.py’

llama.cpp/convert_h 100%[===================&gt;] 409.63K  --.-KB/s    in 0.003s

2025-09-07 16:06:57 (121 MB/s) - ‘llama.cpp/convert_hf_to_gguf.py’ saved [419460/419460]

</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.
We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.
To force `safe_serialization`, set it to `None` instead.
Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded
model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.
Unsloth: Will remove a cached repo with size 2.4G
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 3.59 out of 12.67 RAM for saving.
Unsloth: Saving model... This might take 5 minutes ...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|██████████| 28/28 [00:00&lt;00:00, 28.42it/s]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Unsloth: Saving tokenizer... Done.
Unsloth: Saving /content/drive/MyDrive/lora_model/q4_k_m/pytorch_model-00001-of-00002.bin...
Unsloth: Saving /content/drive/MyDrive/lora_model/q4_k_m/pytorch_model-00002-of-00002.bin...
Done.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Unsloth: Converting llama model. Can use fast conversion = False.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
==((====))==  Unsloth: Conversion from QLoRA to GGUF information
   \\   /|    [0] Installing llama.cpp might take 3 minutes.
O^O/ \_/ \    [1] Converting HF to GGUF 16bits might take 3 minutes.
\        /    [2] Converting GGUF 16bits to [&#39;q4_k_m&#39;] might take 10 minutes each.
 &#34;-____-&#34;     In total, you will have to wait at least 16 minutes.

Unsloth: Installing llama.cpp. This might take 3 minutes...
Unsloth: [1] Converting model at /content/drive/MyDrive/lora_model/q4_k_m into f16 GGUF format.
The output location will be /content/drive/MyDrive/lora_model/q4_k_m/unsloth.F16.gguf
This might take 3 minutes...
INFO:hf-to-gguf:Loading model: q4_k_m
INFO:hf-to-gguf:Model architecture: LlamaForCausalLM
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --&gt; F32, shape = {64}
INFO:hf-to-gguf:gguf: loading model weight map from &#39;pytorch_model.bin.index.json&#39;
INFO:hf-to-gguf:gguf: loading model part &#39;pytorch_model-00001-of-00002.bin&#39;
INFO:hf-to-gguf:token_embd.weight,           torch.float16 --&gt; F16, shape = {3072, 128256}
INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:gguf: loading model part &#39;pytorch_model-00002-of-00002.bin&#39;
INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --&gt; F16, shape = {3072, 3072}
INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --&gt; F16, shape = {3072, 8192}
INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --&gt; F16, shape = {8192, 3072}
INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:output_norm.weight,          torch.float16 --&gt; F32, shape = {3072}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 131072
INFO:hf-to-gguf:gguf: embedding length = 3072
INFO:hf-to-gguf:gguf: feed forward length = 8192
INFO:hf-to-gguf:gguf: head count = 24
INFO:hf-to-gguf:gguf: key-value head count = 8
INFO:hf-to-gguf:gguf: rope theta = 500000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model quantization version
INFO:hf-to-gguf:Set model tokenizer
INFO:numexpr.utils:NumExpr defaulting to 2 threads.
WARNING:gguf.vocab:Unknown separator token &#39;&lt;|begin_of_text|&gt;&#39; in TemplateProcessing&lt;pair&gt;
INFO:gguf.vocab:Adding 280147 merge(s).
INFO:gguf.vocab:Setting special token type bos to 128000
INFO:gguf.vocab:Setting special token type eos to 128001
INFO:gguf.vocab:Setting special token type pad to 128004
INFO:gguf.vocab:Setting add_bos_token to True
INFO:gguf.vocab:Setting add_sep_token to False
INFO:gguf.vocab:Setting chat_template to {{- bos_token }}
{%- if custom_tools is defined %}
    {%- set tools = custom_tools %}
{%- endif %}
{%- if not tools_in_user_message is defined %}
    {%- set tools_in_user_message = true %}
{%- endif %}
{%- if not date_string is defined %}
    {%- set date_string = &#34;26 July 2024&#34; %}
{%- endif %}
{%- if not tools is defined %}
    {%- set tools = none %}
{%- endif %}

{#- This block extracts the system message, so we can slot it into the right place. #}
{%- if messages[0][&#39;role&#39;] == &#39;system&#39; %}
    {%- set system_message = messages[0][&#39;content&#39;] %}
    {%- set messages = messages[1:] %}
{%- else %}
    {%- set system_message = &#34;&#34; %}
{%- endif %}

{#- System message + builtin tools #}
{{- &#34;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;

&#34; }}
{%- if builtin_tools is defined or tools is not none %}
    {{- &#34;Environment: ipython
&#34; }}
{%- endif %}
{%- if builtin_tools is defined %}
    {{- &#34;Tools: &#34; + builtin_tools | reject(&#39;equalto&#39;, &#39;code_interpreter&#39;) | join(&#34;, &#34;) + &#34;

&#34;}}
{%- endif %}
{{- &#34;Cutting Knowledge Date: December 2023
&#34; }}
{{- &#34;Today Date: &#34; + date_string + &#34;

&#34; }}
{%- if tools is not none and not tools_in_user_message %}
    {{- &#34;You have access to the following functions. To call a function, please respond with JSON for a function call.&#34; }}
    {{- &#39;Respond in the format {&#34;name&#34;: function name, &#34;parameters&#34;: dictionary of argument name and its value}.&#39; }}
    {{- &#34;Do not use variables.

&#34; }}
    {%- for t in tools %}
        {{- t | tojson(indent=4) }}
        {{- &#34;

&#34; }}
    {%- endfor %}
{%- endif %}
{{- system_message }}
{{- &#34;&lt;|eot_id|&gt;&#34; }}

{#- Custom tools are passed in a user message with some extra guidance #}
{%- if tools_in_user_message and not tools is none %}
    {#- Extract the first user message so we can plug it in here #}
    {%- if messages | length != 0 %}
        {%- set first_user_message = messages[0][&#39;content&#39;] %}
        {%- set messages = messages[1:] %}
    {%- else %}
        {{- raise_exception(&#34;Cannot put tools in the first user message when there&#39;s no first user message!&#34;) }}
{%- endif %}
    {{- &#39;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;

&#39; -}}
    {{- &#34;Given the following functions, please respond with a JSON for a function call &#34; }}
    {{- &#34;with its proper arguments that best answers the given prompt.

&#34; }}
    {{- &#39;Respond in the format {&#34;name&#34;: function name, &#34;parameters&#34;: dictionary of argument name and its value}.&#39; }}
    {{- &#34;Do not use variables.

&#34; }}
    {%- for t in tools %}
        {{- t | tojson(indent=4) }}
        {{- &#34;

&#34; }}
    {%- endfor %}
    {{- first_user_message + &#34;&lt;|eot_id|&gt;&#34;}}
{%- endif %}

{%- for message in messages %}
    {%- if not (message.role == &#39;ipython&#39; or message.role == &#39;tool&#39; or &#39;tool_calls&#39; in message) %}
        {{- &#39;&lt;|start_header_id|&gt;&#39; + message[&#39;role&#39;] + &#39;&lt;|end_header_id|&gt;

&#39;+ message[&#39;content&#39;] + &#39;&lt;|eot_id|&gt;&#39; }}
    {%- elif &#39;tool_calls&#39; in message %}
        {%- if not message.tool_calls|length == 1 %}
            {{- raise_exception(&#34;This model only supports single tool-calls at once!&#34;) }}
        {%- endif %}
        {%- set tool_call = message.tool_calls[0].function %}
        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}
            {{- &#39;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;

&#39; -}}
            {{- &#34;&lt;|python_tag|&gt;&#34; + tool_call.name + &#34;.call(&#34; }}
            {%- for arg_name, arg_val in tool_call.arguments | items %}
                {{- arg_name + &#39;=&#34;&#39; + arg_val + &#39;&#34;&#39; }}
                {%- if not loop.last %}
                    {{- &#34;, &#34; }}
                {%- endif %}
                {%- endfor %}
            {{- &#34;)&#34; }}
        {%- else  %}
            {{- &#39;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;

&#39; -}}
            {{- &#39;{&#34;name&#34;: &#34;&#39; + tool_call.name + &#39;&#34;, &#39; }}
            {{- &#39;&#34;parameters&#34;: &#39; }}
            {{- tool_call.arguments | tojson }}
            {{- &#34;}&#34; }}
        {%- endif %}
        {%- if builtin_tools is defined %}
            {#- This means we&#39;re in ipython mode #}
            {{- &#34;&lt;|eom_id|&gt;&#34; }}
        {%- else %}
            {{- &#34;&lt;|eot_id|&gt;&#34; }}
        {%- endif %}
    {%- elif message.role == &#34;tool&#34; or message.role == &#34;ipython&#34; %}
        {{- &#34;&lt;|start_header_id|&gt;ipython&lt;|end_header_id|&gt;

&#34; }}
        {%- if message.content is mapping or message.content is iterable %}
            {{- message.content | tojson }}
        {%- else %}
            {{- message.content }}
        {%- endif %}
        {{- &#34;&lt;|eot_id|&gt;&#34; }}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- &#39;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;

&#39; }}
{%- endif %}

INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:/content/drive/MyDrive/lora_model/q4_k_m/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G
Writing: 100%|██████████| 6.43G/6.43G [02:40&lt;00:00, 40.1Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to /content/drive/MyDrive/lora_model/q4_k_m/unsloth.F16.gguf
Unsloth: Conversion completed! Output location: /content/drive/MyDrive/lora_model/q4_k_m/unsloth.F16.gguf
Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...
main: build = 6403 (3b15924d)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu
main: quantizing &#39;/content/drive/MyDrive/lora_model/q4_k_m/unsloth.F16.gguf&#39; to &#39;/content/drive/MyDrive/lora_model/q4_k_m/unsloth.Q4_K_M.gguf&#39; as Q4_K_M using 4 threads
llama_model_loader: loaded meta data with 32 key-value pairs and 255 tensors from /content/drive/MyDrive/lora_model/q4_k_m/unsloth.F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Q4_K_M
llama_model_loader: - kv   3:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   4:                         general.size_label str              = 3.2B
llama_model_loader: - kv   5:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   6:                               general.tags arr[str,2]       = [&#34;unsloth&#34;, &#34;llama.cpp&#34;]
llama_model_loader: - kv   7:                          llama.block_count u32              = 28
llama_model_loader: - kv   8:                       llama.context_length u32              = 131072
llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [&#34;!&#34;, &#34;\&#34;&#34;, &#34;#&#34;, &#34;$&#34;, &#34;%&#34;, &#34;&amp;&#34;, &#34;&#39;&#34;, ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [&#34;Ġ Ġ&#34;, &#34;Ġ ĠĠĠ&#34;, &#34;ĠĠ ĠĠ&#34;, &#34;...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128004
llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  30:               tokenizer.ggml.add_sep_token bool             = false
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type  f16:  197 tensors
[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB
[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB -&gt;   308.23 MiB
[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB -&gt;     5.06 MiB
[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB -&gt;    19.69 MiB
[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB
[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB -&gt;    13.50 MiB
llama_model_quantize_impl: model size  =  6128.17 MB
llama_model_quantize_impl: quant size  =  1918.35 MB

main: quantize time = 376145.88 ms
main:    total time = 376145.88 ms
Unsloth: Conversion completed! Output location: /content/drive/MyDrive/lora_model/q4_k_m/unsloth.Q4_K_M.gguf
Unsloth: Saved Ollama Modelfile to /content/drive/MyDrive/lora_model/q4_k_m/Modelfile
</pre></div></div>
</div>
<script type="application/vnd.jupyter.widget-state+json">
{"04003b7454e749a58f6bbae303531d01": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_45f4c05b32544535b74c8212a8e00425", "IPY_MODEL_4068dabf4e1346d0a368ed3888d20bf6", "IPY_MODEL_fe259f33ff354d7dbdc55e7d708b1f6b"], "layout": "IPY_MODEL_8d1d2a4b94cf40d7a72a1db538637162"}}, "45f4c05b32544535b74c8212a8e00425": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_447fed5a50ff4106897ddb60d4ef6bf1", "placeholder": "\u200b", "style": "IPY_MODEL_3e1513f0d1614eb3b59fe196a19f8d31", "value": "model.safetensors:\u2007100%"}}, "4068dabf4e1346d0a368ed3888d20bf6": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_698141d95e3546eb889979e5ce540e8c", "max": 2354805470, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_9cf6463269a548d39b6f59a2ff58292a", "value": 2354805470}}, "fe259f33ff354d7dbdc55e7d708b1f6b": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_16fd75e3f2a441a2a8713b91d16ca757", "placeholder": "\u200b", "style": "IPY_MODEL_b76fe804020f4db48ddb5f4a931a7108", "value": "\u20072.35G/2.35G\u2007[00:22&lt;00:00,\u200796.8MB/s]"}}, "8d1d2a4b94cf40d7a72a1db538637162": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "447fed5a50ff4106897ddb60d4ef6bf1": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3e1513f0d1614eb3b59fe196a19f8d31": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "698141d95e3546eb889979e5ce540e8c": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9cf6463269a548d39b6f59a2ff58292a": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "16fd75e3f2a441a2a8713b91d16ca757": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b76fe804020f4db48ddb5f4a931a7108": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7a1311df3aab46efb46bc8b82426b410": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_24aaf54693154da7990faa7d09489da9", "IPY_MODEL_eec097b9a5954df58715ab5b4b5e6ce5", "IPY_MODEL_1ea8f067ed1d4dbeb12387079f719b96"], "layout": "IPY_MODEL_11f2278641ff477a9ab479310bfc7bce"}}, "24aaf54693154da7990faa7d09489da9": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_185863b51ad04cdf9b34fb30a259c34b", "placeholder": "\u200b", "style": "IPY_MODEL_faad5746abaf4a8eb5dea6e6ee30d2d8", "value": "generation_config.json:\u2007100%"}}, "eec097b9a5954df58715ab5b4b5e6ce5": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_dfb517a3945949999344eb0b03315139", "max": 234, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_7e086cc46b8041db84d12589d25fed44", "value": 234}}, "1ea8f067ed1d4dbeb12387079f719b96": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_75dd79d6aa1f4dbc872c17622dd2d857", "placeholder": "\u200b", "style": "IPY_MODEL_25b1034f63a543dc9a1b14feba2a1deb", "value": "\u2007234/234\u2007[00:00&lt;00:00,\u200711.5kB/s]"}}, "11f2278641ff477a9ab479310bfc7bce": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "185863b51ad04cdf9b34fb30a259c34b": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "faad5746abaf4a8eb5dea6e6ee30d2d8": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "dfb517a3945949999344eb0b03315139": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7e086cc46b8041db84d12589d25fed44": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "75dd79d6aa1f4dbc872c17622dd2d857": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "25b1034f63a543dc9a1b14feba2a1deb": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "8f09b80b9ca940ee847c154fb250794d": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_59c8d4279a3e4e6285c4c8988ba37fbc", "IPY_MODEL_fc8a96c64b8f49218863e392b6046380", "IPY_MODEL_406046f5038d43ffbce008f0f9c2ccbd"], "layout": "IPY_MODEL_eb266f1b53c14455a668cb049a9860e1"}}, "59c8d4279a3e4e6285c4c8988ba37fbc": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a3d9ea8ab23949d1bf86d7e570d6bee3", "placeholder": "\u200b", "style": "IPY_MODEL_90afcc13c5984d6f84997e2e99840d73", "value": "tokenizer_config.json:\u2007"}}, "fc8a96c64b8f49218863e392b6046380": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_2d34c575dadb48be8b6ec4a619bdfb8b", "max": 1, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_b6d09865f1bc432bab38f90d15a5f971", "value": 1}}, "406046f5038d43ffbce008f0f9c2ccbd": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_4dee21bd26ac4e9c989d1bdc671a8cd0", "placeholder": "\u200b", "style": "IPY_MODEL_78a44d93aa6c4cb7aa9f738b2c225eba", "value": "\u200754.7k/?\u2007[00:00&lt;00:00,\u20074.50MB/s]"}}, "eb266f1b53c14455a668cb049a9860e1": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a3d9ea8ab23949d1bf86d7e570d6bee3": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "90afcc13c5984d6f84997e2e99840d73": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2d34c575dadb48be8b6ec4a619bdfb8b": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "20px"}}, "b6d09865f1bc432bab38f90d15a5f971": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "4dee21bd26ac4e9c989d1bdc671a8cd0": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "78a44d93aa6c4cb7aa9f738b2c225eba": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "882feda1f44647b3be7ca704a87245ae": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_baee9d9f53b04260891166c5625133e0", "IPY_MODEL_49f097b631f746b88eff82d0378d0aa9", "IPY_MODEL_502a97945c8a42558ea6aed31dbdf1bd"], "layout": "IPY_MODEL_42b76f2309784a7aa7c8eb84bbf899f8"}}, "baee9d9f53b04260891166c5625133e0": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_e06e0434f03541088d08c3d55a82ee6f", "placeholder": "\u200b", "style": "IPY_MODEL_846a7493019e42acb104b6eda65b7ce0", "value": "special_tokens_map.json:\u2007100%"}}, "49f097b631f746b88eff82d0378d0aa9": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_54aef4eef5884fca88cb3857baf03d1a", "max": 454, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_de2b6a8ae7f94f7c9b9e00e3d2e515f3", "value": 454}}, "502a97945c8a42558ea6aed31dbdf1bd": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_16d553825315429795fce21016648a52", "placeholder": "\u200b", "style": "IPY_MODEL_afbdd68cae72454c83c6c3196746e4bf", "value": "\u2007454/454\u2007[00:00&lt;00:00,\u200756.9kB/s]"}}, "42b76f2309784a7aa7c8eb84bbf899f8": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e06e0434f03541088d08c3d55a82ee6f": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "846a7493019e42acb104b6eda65b7ce0": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "54aef4eef5884fca88cb3857baf03d1a": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "de2b6a8ae7f94f7c9b9e00e3d2e515f3": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "16d553825315429795fce21016648a52": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "afbdd68cae72454c83c6c3196746e4bf": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7eb1b56493eb46d88f8d8c239ba41a05": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e4e6efdb13b44df3b2137653a343ec03", "IPY_MODEL_6180c9666764483490071bc7a2f7adc2", "IPY_MODEL_5cbb40cbd5c54b5091d03ca2188e7a33"], "layout": "IPY_MODEL_faf46e1da30e4976bab6dee40381ea91"}}, "e4e6efdb13b44df3b2137653a343ec03": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_3fd634b2826a453aa485b333919165bc", "placeholder": "\u200b", "style": "IPY_MODEL_d039af84e0f243de92c9d483689dc607", "value": "tokenizer.json:\u2007100%"}}, "6180c9666764483490071bc7a2f7adc2": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_101221492a05439c994e0c58b8dd6937", "max": 17209920, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_2f24b2d86d9e40e9818b57a8f58f5783", "value": 17209920}}, "5cbb40cbd5c54b5091d03ca2188e7a33": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ae4ec1cab2dd4a4795433ee95ca4dece", "placeholder": "\u200b", "style": "IPY_MODEL_de8854a2ab20417b8880aa8b1ec6324c", "value": "\u200717.2M/17.2M\u2007[00:00&lt;00:00,\u200729.5MB/s]"}}, "faf46e1da30e4976bab6dee40381ea91": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3fd634b2826a453aa485b333919165bc": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d039af84e0f243de92c9d483689dc607": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "101221492a05439c994e0c58b8dd6937": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2f24b2d86d9e40e9818b57a8f58f5783": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "ae4ec1cab2dd4a4795433ee95ca4dece": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "de8854a2ab20417b8880aa8b1ec6324c": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "ec420f89562149f5be73ac515ec2a0d6": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_04412221023f4a008bbba3caeff57541", "IPY_MODEL_fa0beb9125e140689c4e89a029c47a0f", "IPY_MODEL_d7c87f685c22402199b8ad29fb0f4a35"], "layout": "IPY_MODEL_51143bfb324f49949d9bae449ca591f1"}}, "04412221023f4a008bbba3caeff57541": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_74770e343e5b47378807c39ad237825b", "placeholder": "\u200b", "style": "IPY_MODEL_32f069bc4eaa4cbc85bb817ff1265acc", "value": "chat_template.jinja:\u2007"}}, "fa0beb9125e140689c4e89a029c47a0f": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_9ef4e7778e504753a9c82e36c18db6aa", "max": 1, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_df3858f5e10c48e680ec554c1f62fb27", "value": 1}}, "d7c87f685c22402199b8ad29fb0f4a35": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f8e6ac176a264f16b08684059c3a4b9c", "placeholder": "\u200b", "style": "IPY_MODEL_626b1b54817a4bd680a6ee134dc94a4f", "value": "\u20073.83k/?\u2007[00:00&lt;00:00,\u2007230kB/s]"}}, "51143bfb324f49949d9bae449ca591f1": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "74770e343e5b47378807c39ad237825b": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "32f069bc4eaa4cbc85bb817ff1265acc": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9ef4e7778e504753a9c82e36c18db6aa": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "20px"}}, "df3858f5e10c48e680ec554c1f62fb27": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "f8e6ac176a264f16b08684059c3a4b9c": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "626b1b54817a4bd680a6ee134dc94a4f": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "69dad75c802f4bb990b2774a15ce8f96": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_9d76574e6e944b54b938f96768ec7d3f", "IPY_MODEL_947f4a603e864d3b96967e6decf9ef83", "IPY_MODEL_6be2fad90b964374a482989c9cb37f98"], "layout": "IPY_MODEL_e468809bb24f434cafc8d910eac9c76b"}}, "9d76574e6e944b54b938f96768ec7d3f": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_c18a45ce33cd45a9a99fe730ab5fd6ef", "placeholder": "\u200b", "style": "IPY_MODEL_abd12c3d685c402fbdeb715e95376e70", "value": "Generating\u2007train\u2007split:\u2007"}}, "947f4a603e864d3b96967e6decf9ef83": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_61eea50ae7044718badc09b6dd6f65e9", "max": 1, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_a4e8463ac98d4b4c8b507881494dd086", "value": 1}}, "6be2fad90b964374a482989c9cb37f98": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_476ceea4027b4442b0eff286514a2dd8", "placeholder": "\u200b", "style": "IPY_MODEL_32815e3cf57342f6a5a0adf5ad72253a", "value": "\u200774/0\u2007[00:00&lt;00:00,\u2007428.13\u2007examples/s]"}}, "e468809bb24f434cafc8d910eac9c76b": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c18a45ce33cd45a9a99fe730ab5fd6ef": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "abd12c3d685c402fbdeb715e95376e70": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "61eea50ae7044718badc09b6dd6f65e9": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "20px"}}, "a4e8463ac98d4b4c8b507881494dd086": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "476ceea4027b4442b0eff286514a2dd8": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "32815e3cf57342f6a5a0adf5ad72253a": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "db732be82b784dca8630f5885cf3bcd5": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_7753aca1fd864671b5d77bbf882cd5b1", "IPY_MODEL_63df4a20ef364e13873af901dc3b08dd", "IPY_MODEL_f62990538a2d4693835f6a1cb8b9ea96"], "layout": "IPY_MODEL_cd9d970610b346a6846b7e2ccde9febb"}}, "7753aca1fd864671b5d77bbf882cd5b1": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_dfd9624ab2a5441199a78a49b788b90d", "placeholder": "\u200b", "style": "IPY_MODEL_0a35e9f6a73749d092d146200f7e29fa", "value": "Map:\u2007100%"}}, "63df4a20ef364e13873af901dc3b08dd": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_84011f62d18744afb0ee13a2a5dd3d33", "max": 74, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_65650775fa23401f9e67e9c0ae479109", "value": 74}}, "f62990538a2d4693835f6a1cb8b9ea96": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_373fdd75c63248c1980bef8dd7502fee", "placeholder": "\u200b", "style": "IPY_MODEL_d16a9d79491a4a118b181455a341f541", "value": "\u200774/74\u2007[00:00&lt;00:00,\u2007698.92\u2007examples/s]"}}, "cd9d970610b346a6846b7e2ccde9febb": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "dfd9624ab2a5441199a78a49b788b90d": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0a35e9f6a73749d092d146200f7e29fa": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "84011f62d18744afb0ee13a2a5dd3d33": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "65650775fa23401f9e67e9c0ae479109": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "373fdd75c63248c1980bef8dd7502fee": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d16a9d79491a4a118b181455a341f541": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "c3b32ab5a48c4b3cbe595d8b4054d2da": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_f13fd98ad78a41829aed783c80c92f25", "IPY_MODEL_093a836f52d44da69da1b1fbe7c416c1", "IPY_MODEL_aa564a4dc81846d887e9bdffb7a3f5ec"], "layout": "IPY_MODEL_60dad5a2bcbd4c93898bf1ebba47c6b1"}}, "f13fd98ad78a41829aed783c80c92f25": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_613044e83d7a49e6bc2636c9d1aca7cf", "placeholder": "\u200b", "style": "IPY_MODEL_5f2966ed7b0a4df19980b9dcf88922b0", "value": "Unsloth:\u2007Standardizing\u2007formats\u2007(num_proc=2):\u2007100%"}}, "093a836f52d44da69da1b1fbe7c416c1": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d2d91e57dcef46bd8fc5b43ece80e8de", "max": 74, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_b0a6191515e040138c13f374ae1e7e78", "value": 74}}, "aa564a4dc81846d887e9bdffb7a3f5ec": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_69388232bd7645a198783182248d90c7", "placeholder": "\u200b", "style": "IPY_MODEL_b41375c8713d4ff088d04b15b458bcc3", "value": "\u200774/74\u2007[00:00&lt;00:00,\u200767.32\u2007examples/s]"}}, "60dad5a2bcbd4c93898bf1ebba47c6b1": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "613044e83d7a49e6bc2636c9d1aca7cf": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5f2966ed7b0a4df19980b9dcf88922b0": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "d2d91e57dcef46bd8fc5b43ece80e8de": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b0a6191515e040138c13f374ae1e7e78": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "69388232bd7645a198783182248d90c7": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b41375c8713d4ff088d04b15b458bcc3": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "8359d948670c449ea331465c5f2c5d0a": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_665cc002040948e39f3ba890dfdea1e1", "IPY_MODEL_0f669d3557e940dda981085446045872", "IPY_MODEL_1ae84b68a6b24706974fb9262a640344"], "layout": "IPY_MODEL_f55984283b5f4e25b06d1486497573c9"}}, "665cc002040948e39f3ba890dfdea1e1": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a7b45e3a358040a4a12fc123ef5f105a", "placeholder": "\u200b", "style": "IPY_MODEL_8d94fbbbd7a449a4878b5d86c5245b5a", "value": "tokenizer_config.json:\u2007"}}, "0f669d3557e940dda981085446045872": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_7a44f4dde16640d2ba2b3d7d79d07657", "max": 1, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_adb9fdf23703487dab82f2c6c6948bdb", "value": 1}}, "1ae84b68a6b24706974fb9262a640344": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_9a47e4c160374827b4c7bfac742e0a89", "placeholder": "\u200b", "style": "IPY_MODEL_dc5a0c19a77a46b7a77eb1a336d1869c", "value": "\u200750.6k/?\u2007[00:00&lt;00:00,\u2007818kB/s]"}}, "f55984283b5f4e25b06d1486497573c9": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a7b45e3a358040a4a12fc123ef5f105a": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8d94fbbbd7a449a4878b5d86c5245b5a": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7a44f4dde16640d2ba2b3d7d79d07657": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "20px"}}, "adb9fdf23703487dab82f2c6c6948bdb": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "9a47e4c160374827b4c7bfac742e0a89": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "dc5a0c19a77a46b7a77eb1a336d1869c": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "507e6f84ff9c48b8af8d9562a467ecb6": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_5a03767c44264cea8349e10c567ce243", "IPY_MODEL_5cc01b8f82fe43509d5a17d43522548c", "IPY_MODEL_7a42e44963eb49d0967ebc9fc0490ab6"], "layout": "IPY_MODEL_9a91233f311f426ca3e2ebfe6f5e6527"}}, "5a03767c44264cea8349e10c567ce243": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_72c83b35ee1945fca9b8f8d5d1e7b65d", "placeholder": "\u200b", "style": "IPY_MODEL_050658d3dab4431bb4f0e887de0685c4", "value": "tokenizer.json:\u2007100%"}}, "5cc01b8f82fe43509d5a17d43522548c": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_073abc0b957e451c9a1531c739785ba4", "max": 17209920, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_f102ed3cafa44064b5b5234816e46ddb", "value": 17209920}}, "7a42e44963eb49d0967ebc9fc0490ab6": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_2a4520ab077546b9a00ea2541dbd7824", "placeholder": "\u200b", "style": "IPY_MODEL_ba48cab40a204628acc9137e156e0c27", "value": "\u200717.2M/17.2M\u2007[00:00&lt;00:00,\u200725.6MB/s]"}}, "9a91233f311f426ca3e2ebfe6f5e6527": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "72c83b35ee1945fca9b8f8d5d1e7b65d": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "050658d3dab4431bb4f0e887de0685c4": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "073abc0b957e451c9a1531c739785ba4": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f102ed3cafa44064b5b5234816e46ddb": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "2a4520ab077546b9a00ea2541dbd7824": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ba48cab40a204628acc9137e156e0c27": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "22af7643dd6a492f843a359195d4d0c8": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_261649e784ce48a78bec17a30f90b3f2", "IPY_MODEL_dc91f936f529475da053452283d8d2f5", "IPY_MODEL_7dec34798aac45879b16548a53190632"], "layout": "IPY_MODEL_4e079d87ae8c40269f5c7c5c8b1db345"}}, "261649e784ce48a78bec17a30f90b3f2": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_bad6fd9083094c999ecd0abe04b924f0", "placeholder": "\u200b", "style": "IPY_MODEL_244824416f614d0784987020e355c738", "value": "special_tokens_map.json:\u2007100%"}}, "dc91f936f529475da053452283d8d2f5": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_bf9375b4a0f640fe878c5eaac7debe8e", "max": 459, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_48c87d0db48c48ed82760c99080fade1", "value": 459}}, "7dec34798aac45879b16548a53190632": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_b5c6b256b4774574a48f61b73db44d21", "placeholder": "\u200b", "style": "IPY_MODEL_8c13ef4938634f3bb4e89af2c7718bdf", "value": "\u2007459/459\u2007[00:00&lt;00:00,\u20078.24kB/s]"}}, "4e079d87ae8c40269f5c7c5c8b1db345": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "bad6fd9083094c999ecd0abe04b924f0": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "244824416f614d0784987020e355c738": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "bf9375b4a0f640fe878c5eaac7debe8e": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "48c87d0db48c48ed82760c99080fade1": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "b5c6b256b4774574a48f61b73db44d21": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8c13ef4938634f3bb4e89af2c7718bdf": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "4ed46481cb0249f49545135d1549d906": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_bc733a881ec8424e95cd853cd6933d1e", "IPY_MODEL_17844ae246fa4c359125a10d7fa1cb54", "IPY_MODEL_38d0d8009e2548b7adec10f4746f26d6"], "layout": "IPY_MODEL_f43d2f801bcc4a13931e699cafa63213"}}, "bc733a881ec8424e95cd853cd6933d1e": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_531d2971501548abbb6cd55095cfd8eb", "placeholder": "\u200b", "style": "IPY_MODEL_f24ae6bbaf924cc8a71dadda2d9045b8", "value": "Map:\u2007100%"}}, "17844ae246fa4c359125a10d7fa1cb54": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_9dca379abe86480a9ea708cb5a03ce3b", "max": 74, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_cfaa6820849f4232ba5f8508c5e46398", "value": 74}}, "38d0d8009e2548b7adec10f4746f26d6": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_faa3a1b4d08f483fa1c7ce3f2c6de60f", "placeholder": "\u200b", "style": "IPY_MODEL_bfb142ae0b7c4088a76f04843ad8b126", "value": "\u200774/74\u2007[00:00&lt;00:00,\u2007517.88\u2007examples/s]"}}, "f43d2f801bcc4a13931e699cafa63213": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "531d2971501548abbb6cd55095cfd8eb": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f24ae6bbaf924cc8a71dadda2d9045b8": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9dca379abe86480a9ea708cb5a03ce3b": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "cfaa6820849f4232ba5f8508c5e46398": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "faa3a1b4d08f483fa1c7ce3f2c6de60f": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "bfb142ae0b7c4088a76f04843ad8b126": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "8b812f0405c24060b4d3ae256ad0db78": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_1f27edf0a38f4c5a9237b5100f0cb06f", "IPY_MODEL_8e38358e3f2a4f32963285e8fe460bd8", "IPY_MODEL_3c9c4eb6b5c94ea3a43f4cea215f3908"], "layout": "IPY_MODEL_6d59d6dde83148b5af363b66bfecbe98"}}, "1f27edf0a38f4c5a9237b5100f0cb06f": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d63d2bdec7084caeb9e9540f93275e20", "placeholder": "\u200b", "style": "IPY_MODEL_3757a8398adf4b0b9624f82c7eac9138", "value": "Unsloth:\u2007Tokenizing\u2007[&quot;text&quot;]\u2007(num_proc=6):\u2007100%"}}, "8e38358e3f2a4f32963285e8fe460bd8": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_fd01830fb32a4f73b204fcd23ee67fce", "max": 74, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_8b9ecd326380414da25c3a5c9fa329e4", "value": 74}}, "3c9c4eb6b5c94ea3a43f4cea215f3908": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_632f09d83e704049a49672f1167419c3", "placeholder": "\u200b", "style": "IPY_MODEL_306acfa6aec14f78bc22d08994eaee5a", "value": "\u200774/74\u2007[00:11&lt;00:00,\u200711.15\u2007examples/s]"}}, "6d59d6dde83148b5af363b66bfecbe98": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d63d2bdec7084caeb9e9540f93275e20": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3757a8398adf4b0b9624f82c7eac9138": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "fd01830fb32a4f73b204fcd23ee67fce": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8b9ecd326380414da25c3a5c9fa329e4": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "632f09d83e704049a49672f1167419c3": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "306acfa6aec14f78bc22d08994eaee5a": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "cebf494ade2e4b47b3a076731c3e3ce7": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_52066e433d2b46d89a458669de14d4ea", "IPY_MODEL_5137d6db0c1b45b091359569b6f5ff70", "IPY_MODEL_d5e923d8308f48f6b7d640ce87f0323e"], "layout": "IPY_MODEL_c12fc43f61c14f57bdd1e8555e1b57f5"}}, "52066e433d2b46d89a458669de14d4ea": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_1fc35aa7efbf4cbb9c27bf9cb8f371b5", "placeholder": "\u200b", "style": "IPY_MODEL_9ca61bdea62d468a9546bd0abbd04cf8", "value": "Map\u2007(num_proc=2):\u2007100%"}}, "5137d6db0c1b45b091359569b6f5ff70": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f5cceaf0c4e74b4a9329bbe1172d7e94", "max": 74, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_933bcf97c6cc45b09e3b7e2de7d74a05", "value": 74}}, "d5e923d8308f48f6b7d640ce87f0323e": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_aff48ac3e33e45de8770b4ffc547ebb1", "placeholder": "\u200b", "style": "IPY_MODEL_0410bcd3596446bb868093ba8a0b9d67", "value": "\u200774/74\u2007[00:00&lt;00:00,\u2007139.44\u2007examples/s]"}}, "c12fc43f61c14f57bdd1e8555e1b57f5": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1fc35aa7efbf4cbb9c27bf9cb8f371b5": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9ca61bdea62d468a9546bd0abbd04cf8": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "f5cceaf0c4e74b4a9329bbe1172d7e94": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "933bcf97c6cc45b09e3b7e2de7d74a05": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "aff48ac3e33e45de8770b4ffc547ebb1": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0410bcd3596446bb868093ba8a0b9d67": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}}
</script></section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="day-14.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">【Day14】使用 fastmcp 進行 MCP 開發</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#基本設定">基本設定</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#連接-Google-Drive">連接 Google Drive</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#安裝套件">安裝套件</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#使用-Unsloth-載入並初始化-4-bit-模型">使用 Unsloth 載入並初始化 4-bit 模型</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#載入-PEFT">載入 PEFT</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#資料集準備">資料集準備</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#訓練模型">訓練模型</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#訓練參數設定">訓練參數設定</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#資源查看">資源查看</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#開始微調">開始微調</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#查看花費時間">查看花費時間</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#運行微調後的模型結果">運行微調後的模型結果</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#儲存微調後的模型">儲存微調後的模型</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#轉換成-GGUF">轉換成 GGUF</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hsiang-Jen Li
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Hsiang-Jen Li.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>